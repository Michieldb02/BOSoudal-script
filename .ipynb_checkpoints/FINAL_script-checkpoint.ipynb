{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6de106b9",
   "metadata": {},
   "source": [
    "<h1>Model 0<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9467e8d6-9962-4310-aa67-c5224dc2aafe",
   "metadata": {},
   "source": [
    "# import libraries and make customer cluster nodes\n",
    "## Start of Model 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fb65815-1f9a-4d6a-a6d4-697fd40625e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michi\\miniconda3\\lib\\site-packages\\geopandas\\_compat.py:106: UserWarning: The Shapely GEOS version (3.11.3-CAPI-1.17.3) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:/Datasets/Soudal data/Customer_orders_points_07052024.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m product\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Load your dataset and remove NA values\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m customer_orders_points_07052024 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mE:/Datasets/Soudal data/Customer_orders_points_07052024.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m customer_id \u001b[38;5;241m=\u001b[39m customer_orders_points_07052024[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOSTCODE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m     16\u001b[0m order_quantity \u001b[38;5;241m=\u001b[39m customer_orders_points_07052024[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantity_Delivered\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna()\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 934\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1229\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 786\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:/Datasets/Soudal data/Customer_orders_points_07052024.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import igraph as ig\n",
    "import geopy.distance\n",
    "import folium\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "import networkx as nx\n",
    "from geopy.distance import geodesic\n",
    "from itertools import product\n",
    "# Load your dataset and remove NA values\n",
    "customer_orders_points_07052024 = pd.read_csv(\"C:/Users/michi/Documents/Blok 2 Jaar 4/Customer_orders_points_07052024.csv\")\n",
    "customer_id = customer_orders_points_07052024['POSTCODE'].dropna()\n",
    "order_quantity = customer_orders_points_07052024['Quantity_Delivered'].dropna()\n",
    "latitude = customer_orders_points_07052024['y'].dropna()\n",
    "latitude = latitude[(latitude <= 90) & (latitude >= -90)]\n",
    "longitude = customer_orders_points_07052024['x'].dropna()\n",
    "longitude = longitude[(longitude <= 90) & (longitude >= -90)]\n",
    "min_length = min(len(longitude), len(latitude))  # min(len(longitude), len(latitude)) or any other criteria\n",
    "# Standardize all vector lengths\n",
    "customer_id = customer_id.head(min_length)\n",
    "order_quantity = order_quantity.head(min_length)\n",
    "latitude = latitude.head(min_length)\n",
    "longitude = longitude.head(min_length)\n",
    "# Create DataFrame from customer order data\n",
    "customer_data = pd.DataFrame({\n",
    "    'Customer': customer_id,\n",
    "    'Latitude': latitude,\n",
    "    'Longitude': longitude,\n",
    "    'Order_quantity': order_quantity\n",
    "})\n",
    "# Create grouped dataset\n",
    "grouped_data = customer_data.groupby(['Latitude', 'Longitude', 'Order_quantity']).agg(Num_Customers=('Customer', 'nunique')).reset_index()\n",
    "# Combine orders per location\n",
    "combined_orders = grouped_data.groupby(['Latitude', 'Longitude']).agg(\n",
    "    Total_Order_Quantity=('Order_quantity', 'sum'),\n",
    "    Total_Num_Customers=('Num_Customers', 'sum')\n",
    ").reset_index()\n",
    "# Perform clustering using DBSCAN on combined_orders dataset based on geoposition only\n",
    "eps = 0.62  # Adjust epsilon (neighborhood distance) as needed\n",
    "minPts = 39  # Adjust minPts (minimum number of points in a cluster) as needed\n",
    "dbscan = DBSCAN(eps=eps, min_samples=minPts, metric='euclidean')\n",
    "dbscan_result = dbscan.fit_predict(combined_orders[['Longitude', 'Latitude']])\n",
    "# Assign cluster labels to data points\n",
    "combined_orders['Cluster'] = dbscan_result\n",
    "# Calculate cluster centers\n",
    "cluster_centers_df = combined_orders.groupby('Cluster').agg(\n",
    "    Longitude=('Longitude', 'mean'),\n",
    "    Latitude=('Latitude', 'mean'),\n",
    "    Total_Order_Quantity=('Total_Order_Quantity', 'sum'),\n",
    "    Count=('Latitude', 'size')\n",
    ").reset_index()\n",
    "cluster_centers_df['ID'] = ['Cluster_' + str(i + 1) for i in range(len(cluster_centers_df))]\n",
    "customer_nodes = cluster_centers_df[['ID', 'Total_Order_Quantity', 'Latitude', 'Longitude']]\n",
    "# Create a dataframe\n",
    "df = pd.DataFrame({\n",
    "    'ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'Country': [\"Portugal\", \"Spain\", \"Italy\", \"France\", \"Belgium\", \"Latvia\", \"Poland\", \"Portugal\", \"Slovenia\", \"Spain\", \"Turkey\", \"Belgium\"],\n",
    "    'City': [\"Lissabon\", \"Alovera\", \"Cologno Monzese\", \"Blyes\", \"Turnhout\", \"Dobele\", \"Pionki\", \"Lissabon\", \"Srpenica\", \"Alovera\", \"Tuzla, Istanbul\", \"Pelt\"],\n",
    "    'Plant_type': [\"Receiving plant\", \"Receiving plant\", \"Receiving plant\", \"Receiving plant\", \"Producing plant\", \"Producing plant\", \"Producing plant\", \"Producing plant\", \"Producing plant\", \"Producing plant\", \"Producing plant\", \"Producing plant\"],\n",
    "    'ZIP_code': [\"2710-207\", \"19208\", \"20093\", \"1150\", \"2300\", \"3701\", \"26-670\", \"2710-207\", \"5224\", \"19208\", \"34956\", \"3900\"],\n",
    "    'Receiving_plant': [2200, 1900, 2000, 1800, None, None, None, None, None, None, None, None],\n",
    "    'X_coordinate': [-9.369216, -3.243162431, 9.281575673, 5.261173337, 4.928540898, 23.30237845, 21.45811508, -9.369216, 13.5166732, -3.243162431, 29.36691489, 5.3966],\n",
    "    'Y_coordinate': [38.757301, 40.57675934, 45.54113003, 45.83747369, 51.29525785, 56.62518423, 51.46902482, 38.757301, 46.28409136, 40.57675934, 40.8858807, 51.1913],\n",
    "    'Geometry': [\"POINT (-9.36922 38.75730)\", \"POINT (-3.24316 40.57676)\", \"POINT (9.28158 45.54113)\", \"POINT (5.26117 45.83747)\", \"POINT (4.92854 51.29526)\", \"POINT (23.30238 56.62518)\", \"POINT (21.45812 51.46902)\", \"POINT (-9.36922 38.75730)\", \"POINT (13.51667 46.28409)\", \"POINT (-3.24316 40.57676)\", \"POINT (29.36691 40.88588)\", \"POINT (5.3966,51.1913)\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578c3366-5f48-4af2-a266-17a1762d5552",
   "metadata": {},
   "source": [
    "# Make warehouse nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed88f7-eafa-4ca5-a6a4-8a24e26f88ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the interplant orders\n",
    "Interplant_Orders_2023_2 = pd.read_csv(\"C:/Users/michi/Documents/Blok 2 Jaar 4/Interplant_Orders_2023_2.csv\")\n",
    "# Assign an easier name to the variable\n",
    "Order_Ref = Interplant_Orders_2023_2\n",
    "# Split the aggregated data into producing plants and receiving plants\n",
    "producing_plants = df[df['Plant_type'] == \"Producing plant\"]\n",
    "receiving_plants = df[df['Plant_type'] == \"Receiving plant\"]\n",
    "# Create a new dataframe with the desired structure\n",
    "new_df = pd.DataFrame({\n",
    "    'ID': df['City'] + \" plant \" + df['ID'].astype(str),\n",
    "    'Total_Quantity': df['Receiving_plant'],\n",
    "    'Latitude': df['Y_coordinate'],\n",
    "    'Longitude': df['X_coordinate']\n",
    "})\n",
    "# Convert Total_Quantity to integer\n",
    "new_df['Total_Quantity'] = new_df['Total_Quantity'].astype(pd.Int64Dtype())\n",
    "# # Group the data by warehouse and calculate both total capacity and frequency\n",
    "# warehouse_capacity = Order_Ref.groupby('Producing_Plant').agg(\n",
    "#     Total_Capacity=('Quantity_Delivered', 'sum'),\n",
    "#     Frequency=('Producing_Plant', 'count')\n",
    "# ).reset_index()\n",
    "# # Create a lookup table mapping country names to country codes\n",
    "# country_lookup_producing = pd.DataFrame({\n",
    "#     'Country': [\"Belgium\", \"Latvia\", \"Poland\", \"Portugal\", \"Slovenia\", \"Spain\", \"Turkey\"],\n",
    "#     'Country_Code': [\"BE\", \"LV\", \"PL\", \"PT\", \"SI\", \"ES\", \"TR\"]\n",
    "# })\n",
    "# # Create a lookup table mapping country names to country codes\n",
    "# country_lookup_receiving = pd.DataFrame({\n",
    "#     'Country': [\"Lissabon\", \"Alovera\", \"Cologno Monzese\", \"Byles\"],\n",
    "#     'Country_Code': [\"PT\", \"ES\", \"IT\", \"FR\"]\n",
    "# })\n",
    "# # Replace the country names with country codes in the producing dataframe\n",
    "# producing_plants['Country'] = producing_plants['Country'].map(dict(zip(country_lookup_producing['Country'], country_lookup_producing['Country_Code'])))\n",
    "# # Replace the country names with country codes in the receiving dataframe\n",
    "# receiving_plants['Country'] = [\"PT\", \"ES\", \"IT\", \"FR\"]\n",
    "# # Grouping by Producing_Plant and Receiving_Plant, and summarizing the Quantity_Delivered and Order_Ref\n",
    "# flow_data = Order_Ref.groupby(['Producing_Plant', 'Receiving_Plant']).agg(\n",
    "#     Total_Quantity=('Quantity_Delivered', 'sum'),\n",
    "#     Frequency=('Producing_Plant', 'count')\n",
    "# ).reset_index()\n",
    "# # Arranging the data by frequency in descending order\n",
    "# flow_data = flow_data.sort_values(by=['Producing_Plant'])\n",
    "# # Assigning country codes and coordinates\n",
    "# flow_data['Country'] = [\"BE\", \"PL\", \"GB\", \"BE\", \"BE\", \"NL\", \"DE\", \"PL\", \"TR\", \"SI\", \"PL\", \"BE\", \"NL\"]\n",
    "# flow_data['X_coordinate'] = [51.3377, 52.3753, 52.6043, 51.1913, 51.1055, 51.2518, 51.0625, 51.4583, 40.6295, 46.2914, 51.4583, 51.1055, 51.5064]\n",
    "# flow_data['Y_coordinate'] = [4.9345, 20.6860, 1.6485, 5.3966, 3.8589, 5.6885, 6.9419, 21.4481, 30.6428, 13.4568, 21.4481, 3.8589, 4.2572]\n",
    "# flow_data.columns = [\"Producing_Plant\", \"Receiving_Plant\", \"Total Quantity\", \"Frequency\", \"Country\", \"Latitude\", \"Longitude\"]\n",
    "# # Assigning IDs\n",
    "# flow_data['ID'] = [\"Turnhout plant 1000\", \"Dąbrówka plant 1030\", \"Great_Britain plant 1100\", \"Pelt plant 1400\", \"Gent plant 1500\",\n",
    "#                    \"Weert plant 1600\", \"Leverkussen plant 1700\", \"Pionki plant 2400\", \"Turkey Plant 5200\", \"Srpenica plant 5600\",\n",
    "#                    \"Pionki plant 5900\", \"Gent plant 6700\", \"Bergen_op_zoom plant 8700\"]\n",
    "# # Selecting specific columns\n",
    "# logistical_flow_nodes = flow_data[['ID', 'Total Quantity', 'Latitude', 'Longitude']]\n",
    "logistical_nodes = new_df[['ID', 'Latitude', 'Longitude']]\n",
    "# Given plant dataset\n",
    "plant_dataset = logistical_nodes\n",
    "# Given customer dataset\n",
    "customer_dataset = pd.DataFrame({\n",
    "    'ID': cluster_centers_df['ID'],\n",
    "    'Longitude': cluster_centers_df['Longitude'],\n",
    "    'Latitude': cluster_centers_df['Latitude']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b479c9db-b7bd-4fb6-b487-b9862d382e05",
   "metadata": {},
   "source": [
    "# Create combined dataset and specify which nodes to feature in the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcf3e49-fac8-446e-91b3-a1d154837b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final combined dataset\n",
    "combined_dataset = pd.concat([new_df[['ID', 'Latitude', 'Longitude']], cluster_centers_df[['ID', 'Latitude', 'Longitude']]], ignore_index=True)\n",
    " \n",
    "# Adjust index to start from 1\n",
    "combined_dataset.index = combined_dataset.index\n",
    " \n",
    "# Calculate distances between each pair of points\n",
    "def calculate_distance(point1, point2):\n",
    "    return geodesic(point1, point2).kilometers\n",
    " \n",
    "distances = np.zeros((len(combined_dataset), len(combined_dataset)))\n",
    " \n",
    "for i, j in product(range(len(combined_dataset)), repeat=2):\n",
    "    distances[i, j] = calculate_distance(\n",
    "        (combined_dataset['Latitude'].iloc[i], combined_dataset['Longitude'].iloc[i]),\n",
    "        (combined_dataset['Latitude'].iloc[j], combined_dataset['Longitude'].iloc[j])\n",
    "    )\n",
    " \n",
    "# Create a graph from the distance matrix\n",
    "g = nx.from_numpy_array(distances, create_using=nx.Graph)\n",
    " \n",
    "def create_map(combined_dataset):\n",
    "    # Create a folium map centered around the average latitude and longitude\n",
    "    avg_lat = combined_dataset['Latitude'].mean()\n",
    "    avg_lon = combined_dataset['Longitude'].mean()\n",
    "    folium_map = folium.Map(location=[avg_lat, avg_lon], zoom_start=5)\n",
    " \n",
    "    # Add markers to the map\n",
    "    for _, row in combined_dataset.iterrows():\n",
    "        if 'Cluster' in row['ID']:\n",
    "            folium.Marker(\n",
    "                location=[row['Latitude'], row['Longitude']],\n",
    "                popup=f\"Cluster: {row['ID']}\",\n",
    "                icon=folium.Icon(color='blue'),\n",
    "                tooltip=row['ID']\n",
    "            ).add_to(folium_map)\n",
    "        else:\n",
    "            folium.Marker(\n",
    "                location=[row['Latitude'], row['Longitude']],\n",
    "                popup=f\"Warehouse: {row['ID']}\",\n",
    "                icon=folium.Icon(color='red'),\n",
    "                tooltip=row['ID']\n",
    "            ).add_to(folium_map)\n",
    "    # Add legend\n",
    "    legend_html = \"\"\"\n",
    "<div style=\"position: fixed; bottom: 50px; right: 50px; z-index:9999; background-color:white; border:2px solid grey; padding: 10px;\">\n",
    "<h4>Legend</h4>\n",
    "<p><span style=\"color:blue\">&#9632;</span> Customer Clusters</p>\n",
    "<p><span style=\"color:red\">&#9632;</span> Warehouses</p>\n",
    "</div>\n",
    "    \"\"\"\n",
    "    folium_map.get_root().html.add_child(folium.Element(legend_html))\n",
    " \n",
    "    # Display the map\n",
    "    display(folium_map)\n",
    " \n",
    "# Display the map\n",
    "create_map(combined_dataset)\n",
    " \n",
    "# Function to select specified nodes manually\n",
    "def select_nodes():\n",
    "    selected_nodes = input(\"Enter the row numbers of the nodes you want to include, separated by commas: \")\n",
    "    selected_nodes = list(map(int, selected_nodes.split(',')))\n",
    "    return selected_nodes\n",
    " \n",
    "# Display the map before selecting nodes\n",
    "display(combined_dataset)\n",
    " \n",
    "# Specify nodes for the objective function\n",
    "specified_nodes = select_nodes()  # The script stops here and waits for user input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81992c0d-d42b-46c5-8878-b956ebfc0f6d",
   "metadata": {},
   "source": [
    "# Define Dijsktra's search objective and execute the algorithm\n",
    "## Following will be outputted a new coordinate alongside a visual map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6367cdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function_dijkstra(coords, specified_nodes, combined_dataset, g):\n",
    "    # Determine the index for the new node\n",
    "    new_node_index = len(combined_dataset)\n",
    "    # Create a temporary graph to add the new node and edges\n",
    "    temp_graph = g.copy()\n",
    "    temp_graph.add_node(new_node_index)\n",
    "    # Calculate the total distance from the new warehouse to each specified node\n",
    "    total_distance = 0\n",
    "    for i in specified_nodes:\n",
    "        distance = calculate_distance(\n",
    "            (coords['Latitude'], coords['Longitude']),\n",
    "            (combined_dataset['Latitude'].iloc[i], combined_dataset['Longitude'].iloc[i])\n",
    "        )\n",
    "        total_distance += distance\n",
    "    return total_distance\n",
    " \n",
    "# Define grid search ranges\n",
    "lon_range = np.linspace(combined_dataset['Longitude'].min(), combined_dataset['Longitude'].max(), num=100)\n",
    "lat_range = np.linspace(combined_dataset['Latitude'].min(), combined_dataset['Latitude'].max(), num=100)\n",
    " \n",
    "grid_search = pd.DataFrame(list(product(lon_range, lat_range)), columns=['Longitude', 'Latitude'])\n",
    " \n",
    "# Calculate objective function values for each grid point\n",
    "objective_values = grid_search.apply(lambda row: objective_function_dijkstra(row, specified_nodes, combined_dataset, g), axis=1)\n",
    " \n",
    "# Find the coordinates of the new position that minimize the objective function\n",
    "optimal_index = np.argmin(objective_values)\n",
    "optimal_coords = grid_search.iloc[optimal_index]\n",
    " \n",
    "# Coordinates of the new position that minimize the total distance\n",
    "new_position_longitude = optimal_coords['Longitude']\n",
    "new_position_latitude = optimal_coords['Latitude']\n",
    " \n",
    "# Sample data\n",
    "# customers = customer_nodes\n",
    "# warehouses = logistical_nodes\n",
    " \n",
    "new_warehouse = pd.DataFrame({\n",
    "    'ID': ['New warehouse'],\n",
    "    'Latitude': [new_position_latitude],\n",
    "    'Longitude': [new_position_longitude]\n",
    "})\n",
    "# Create a folium map\n",
    "m = folium.Map(location=[new_position_latitude, new_position_longitude], zoom_start=10)\n",
    "\n",
    "# Add circle markers for customers\n",
    "for idx, row in customer_dataset.iterrows():\n",
    "    folium.CircleMarker(location=[row['Latitude'], row['Longitude']], radius=5, color='blue', fill=True, fill_color='blue', fill_opacity=0.6, tooltip=row['ID']).add_to(m)\n",
    "\n",
    "# Add circle markers for existing warehouses\n",
    "for idx, row in plant_dataset.iterrows():\n",
    "    folium.CircleMarker(location=[row['Latitude'], row['Longitude']], radius=5, color='red', fill=True, fill_color='red', fill_opacity=0.6, tooltip=row['ID']).add_to(m)\n",
    "\n",
    "# Add circle marker for new warehouse\n",
    "folium.CircleMarker(location=[new_position_latitude, new_position_longitude], radius=5, color='green', fill=True, fill_color='green', fill_opacity=0.6, tooltip='New Warehouse').add_to(m)\n",
    "\n",
    "# Define legend HTML\n",
    "legend_html = '''\n",
    "     <div style=\"position: fixed; \n",
    "                 bottom: 50px; left: 50px; width: 160px; height: 225px; \n",
    "                 border:2px solid grey; z-index:9999; font-size:14px;\n",
    "                 background-color: white;\n",
    "                 \">\n",
    "     <h4 style=\"text-align:center\">Legend</h4>\n",
    "     <p><i class=\"fa fa-circle\" style=\"color:blue\"></i> Central Customer Clusters</p>\n",
    "     <p><i class=\"fa fa-circle\" style=\"color:red\"></i> Existing Soudal Plants</p>\n",
    "     <p><i class=\"fa fa-circle\" style=\"color:green\"></i> Search area for new location</p>\n",
    "      </div>\n",
    "     '''\n",
    "\n",
    "# Add legend to the map\n",
    "m.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# Display the map\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3626d3f2-702e-4e87-8704-46e7d87773a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "318e3168",
   "metadata": {},
   "source": [
    "<h1>Model I<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a075b91",
   "metadata": {},
   "source": [
    "<h3>Loading all raster files and converting them to masked arrays</h3>\n",
    "<blockquote>It's necessary to convert each raster to a masked array in sum and apply custom weights to each criterion. This is done by defining a function which loads each raster and converts the layer. The masked arrays are then returned, which are assigned to a variable named 'masked_arrays'.</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30351ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import show\n",
    "\n",
    "fp_job_vacancy = 'C:/Users/michi/Documents/Blok 2 Jaar 4/Datasets_final_aligned/Job_vacancy_align2.tif'\n",
    "fp_education_level = 'C:/Users/michi/Documents/Blok 2 Jaar 4/Datasets_final_aligned/Education_align.tif'\n",
    "fp_population = 'C:/Users/michi/Documents/Blok 2 Jaar 4/Datasets_final_aligned/Population_align4.tif'\n",
    "fp_unemployement = 'C:/Users/michi/Documents/Blok 2 Jaar 4/Datasets_final_aligned/unemployment_align.tif'\n",
    "fp_harbors =  'C:/Users/michi/Documents/Blok 2 Jaar 4/Datasets_final_aligned/Harbour_distance_align.tif'\n",
    "fp_railways = 'C:/Users/michi/Documents/Blok 2 Jaar 4/Datasets_final_aligned/railway_dock_distance_align.tif'\n",
    "fp_highways = 'C:/Users/michi/Documents/Blok 2 Jaar 4/Datasets_final_aligned/highway_junction_distance_align.tif'\n",
    "fp_solar_energy = 'C:/Users/michi/Documents/Blok 2 Jaar 4/Datasets_final_aligned/solar_energy_align.tif'\n",
    "fp_euro_percapita = 'C:/Users/michi/Documents/Blok 2 Jaar 4/Datasets_final_aligned/Labour_cost_align.tif'\n",
    "fp_laborcosts_ph = 'C:/Users/michi/Documents/Blok 2 Jaar 4/Datasets_final_aligned/labour_cost_hour_align4.tif'\n",
    "fp_protected_nature = 'C:/Users/michi/Documents/Blok 2 Jaar 4/Datasets_final_aligned/Protected_nature_align.tif'\n",
    "fp_waterbodies = 'C:/Users/michi/Documents/Blok 2 Jaar 4/Datasets_final_aligned/Waterbodies_align.tif'\n",
    "fp_industrialzones = 'C:/Users/michi/Documents/Blok 2 Jaar 4/Datasets_final_aligned/Industrial_zones_align.tif'\n",
    "\n",
    "\n",
    "raster_fps = [fp_job_vacancy, fp_education_level, fp_population, fp_unemployement, fp_harbors, fp_railways, fp_highways, fp_solar_energy, fp_euro_percapita, fp_laborcosts_ph, fp_protected_nature, fp_waterbodies, fp_industrialzones]\n",
    "plot_titles = ['Job vacancy rate',  'Education level', 'Population level', 'Unemployement percentage', 'Distance to harbors', 'Distance to railway docks', 'Distance to highway junctions', 'Solar energy potential', 'Euro per capita', 'Labor costs per hour', 'Protected nature areas', 'Waterbodies', 'Industrial zones']\n",
    "def rasterizeandplot():\n",
    "    masked_arrays = []\n",
    "    for rasters_fp, title in zip(raster_fps, plot_titles):\n",
    "        with rasterio.open(rasters_fp) as src:\n",
    "            raster_data = src.read(1)\n",
    "            nodata = -3.40282347e+38\n",
    "\n",
    "            mask = np.isclose(raster_data, nodata)\n",
    "            raster_data_masked = np.ma.masked_array(raster_data, mask)\n",
    "            \n",
    "            plt.imshow(raster_data_masked, cmap='Greens')\n",
    "            plt.colorbar(label='Pixel Value')\n",
    "            plt.title(title)\n",
    "            plt.show()\n",
    "            \n",
    "            masked_arrays.append(raster_data_masked)\n",
    "    \n",
    "    return masked_arrays\n",
    "\n",
    "masked_arrays = rasterizeandplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643cab8c",
   "metadata": {},
   "source": [
    "<h3>Merging and applying custom weights to the criterion</h3>\n",
    "<blockquote>Each criterion gets their own weight based on relative importance. These weights are multiplied by the existing classification values (0-4). Next the values are multiplied by the protected nature reserves and water bodies. These were classified 0 (unsuitable) or 1 (suitable). Finally an output file is created, and the metadata from a previous raster is copied in order to transfer information and save the file.  </blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fea4d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geocube.api.core import make_geocube\n",
    "from geocube.rasterize import rasterize_image\n",
    "from functools import partial\n",
    "from rasterio.enums import MergeAlg\n",
    "\n",
    "merge = ((masked_arrays[0] * 0.6) + \n",
    "             (masked_arrays[1] *  1) + \n",
    "             (masked_arrays[2] * 0.8) + \n",
    "             (masked_arrays[3] * 0.4) +\n",
    "             (masked_arrays[4] *  0.8) +\n",
    "              (masked_arrays[5] * 1) +\n",
    "             (masked_arrays[6] * 0.6) +\n",
    "              (masked_arrays[7] * 0.6) +\n",
    "             (masked_arrays[8] * 0.4) +\n",
    "              (masked_arrays[9] * 0.8))\n",
    "raster_exclude = merge * masked_arrays[10] * masked_arrays[11]\n",
    "              \n",
    "plt.imshow(raster_exclude, cmap='YlGn')\n",
    "plt.colorbar(label='Pixel Value')\n",
    "plt.title('Result with custom weights')\n",
    "plt.show()\n",
    "\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.enums import Resampling\n",
    "\n",
    "output_file = 'C:/Users/michi/Documents/Blok 2 Jaar 4/Datasets_final_aligned/result_custom_weights.tif'\n",
    "\n",
    "# Get metadata from one of the input raster files\n",
    "with rasterio.open('C:/Users/michi/Documents/Blok 2 Jaar 4/Datasets_final_aligned/population_align4.tif') as src:\n",
    "    profile = src.profile\n",
    "\n",
    "# Update metadata if necessary \n",
    "profile.update(\n",
    "    dtype=rasterio.float64,  \n",
    "    count=1,  \n",
    "    compress='lzw',  #\n",
    "    nodata=-3.40282347e+38,  \n",
    ")\n",
    "\n",
    "# Create a new raster file for writing\n",
    "with rasterio.open(output_file, 'w', **profile) as dst:\n",
    "    dst.write(raster_exclude.filled(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03c567e",
   "metadata": {},
   "source": [
    "<h3>Merging and applying custom weights to the criterion</h3>\n",
    "<blockquote>The same principle is applied as in the code above. The only difference being the industrial weights are taken into account.</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0426e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_exclude = merge * masked_arrays[10] * masked_arrays[11] * masked_arrays[12]\n",
    "\n",
    "plt.imshow(raster_exclude, cmap='YlGn')\n",
    "plt.colorbar(label='Pixel Value')\n",
    "plt.title('Result with custom weights including industrial zones')\n",
    "plt.show()\n",
    "\n",
    "output_file = 'C:/Users/michi/Documents/Blok 2 Jaar 4/Datasets_final_aligned/result_custom_weights_industrial_zoning.tif'\n",
    "\n",
    "# Get metadata from one of the input raster files (assuming all have the same metadata)\n",
    "with rasterio.open('C:/Users/michi/Documents/Blok 2 Jaar 4/Datasets_final_aligned/population_align4.tif') as src:\n",
    "    profile = src.profile\n",
    "\n",
    "# Update metadata if necessary \n",
    "profile.update(\n",
    "    dtype=rasterio.float64,  \n",
    "    count=1,  \n",
    "    compress='lzw',  #\n",
    "    nodata=-3.40282347e+38,  \n",
    ")\n",
    "\n",
    "# Create a new raster file for writing\n",
    "with rasterio.open(output_file, 'w', **profile) as dst:\n",
    "    dst.write(raster_exclude.filled(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8df4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "d = {'placeholder': ['centerpoint'], 'geometry': [Point(new_warehouse['Longitude'], new_warehouse['Latitude'])]}\n",
    "singlepoint = gpd.GeoDataFrame(d, crs=\"EPSG:4326\")\n",
    "display(singlepoint)\n",
    "singlepoint = singlepoint.to_crs(3043) \n",
    "singlepoint['geometry'] = singlepoint['geometry'].buffer(100000)\n",
    "display(singlepoint)  \n",
    "singlepoint.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1da449",
   "metadata": {},
   "outputs": [],
   "source": [
    "singlepoint = singlepoint.to_crs(epsg=4326)\n",
    "m = folium.Map(location=[46.021031, 5.499198], zoom_start=15, height=500)\n",
    "folium.GeoJson(singlepoint).add_to(m)\n",
    "m.save(\"map_with_buffer.html\")\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f3166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.mask import mask\n",
    "outRas = 'C:/Users/michi/Documents/Blok 2 Jaar 4/Final_datasets/clippedrasters.tif'\n",
    "\n",
    "with rasterio.open(output_file) as src:\n",
    "    singlepoint=singlepoint.to_crs(src.crs)\n",
    "    out_image, out_transform=mask(src,singlepoint.geometry,crop=True)\n",
    "    out_image[out_image >= 20] = 0\n",
    "    out_image[out_image==0] = np.nan\n",
    "    out_meta=src.meta.copy()\n",
    "    \n",
    "out_meta.update({\n",
    "    \"driver\":\"Gtiff\",\n",
    "    \"height\":out_image.shape[1], \n",
    "    \"width\":out_image.shape[2], \n",
    "    \"transform\":out_transform\n",
    "})\n",
    "              \n",
    "with rasterio.open(outRas,'w',**out_meta) as dst:\n",
    "    dst.write(out_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c6ba2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
